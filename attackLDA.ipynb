{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities,matutils\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import sparse as sp\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findVariationalParams(M,datapath,param,alpha,K):\n",
    "    ''' \n",
    "    Function to determine the variational parameters\n",
    "    Input: \n",
    "    M = DxV integer ndarray where \n",
    "           M(d,v) = Count of word v in document d\n",
    "    alpha = K-vector of floats, shape = (K,) \n",
    "    beta = V-vector of floats, shape = (V,)\n",
    "    \n",
    "    Method: Use eq. 2,3,4 to determine outputs\n",
    "    Refer to Blei et al.(2003) for definition of psi\n",
    "    \n",
    "    Output:\n",
    "    eta: KxV float ndarray\n",
    "    gamma: DxK float ndarray\n",
    "    phi: DxVxK float ndarray \n",
    "         (returning a row_sparse D.V x K matrix phisp) \n",
    "    '''\n",
    "    D,V = M.shape\n",
    "    #K = alpha.shape[0]\n",
    "    eta = np.ones ((K,V))\n",
    "    gamma = np.ones((D,K))\n",
    "    phi = np.ones((D,V,K))\n",
    "    ################### CODE HERE #######################\n",
    "    c_code = \"lda-c/\"\n",
    "    cmd1 = \"rm -r \"+param\n",
    "    # Ignore the os error that will come when no such file exists\n",
    "    cmd2 = \"mkdir \"+param\n",
    "    cmd3 = c_code+\"lda est \"+str(alpha)+\" \"+ str(K) +\" \"+ c_code + \\\n",
    "          \"settings.txt \" + datapath + \" random \" + param\n",
    "    os.system(cmd1)\n",
    "    os.system(cmd2)\n",
    "    os.system(cmd3)\n",
    "    print(\"Reading phi\")\n",
    "    p = np.loadtxt(param+\"/final.phi\")\n",
    "    coords = np.int32(p.T[0:-1,:])\n",
    "    phi = sp.COO(coords,p[:,-1],shape=(D,V,K))\n",
    "    print(\"Reading gamma\")\n",
    "    gamma = np.loadtxt(param+\"/final.gamma\")\n",
    "    print(\"Reading eta\")\n",
    "    eta = np.loadtxt(param+\"/final.beta\")\n",
    "    return eta,gamma,phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessWords(inputPath,corpusfile,stop_words):\n",
    "    '''\n",
    "    Parses all files in the inputPath folder and\n",
    "    returns the word matrix M:DxV of type ndarray(int32).\n",
    "    Also stores the corpus in blei's LDA-C format as \n",
    "    corpusfile (corpusfile is a full path with filename).\n",
    "    Input-specific stopwords also taken as array of strings\n",
    "    '''\n",
    "    porter = PorterStemmer()\n",
    "    docs,docLen=[],0\n",
    "    for path in inputPath:\n",
    "        print(\"Reading data from %s\"%path)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(path+filename,'r') as inp:\n",
    "                print(\"Reading data from %s\"%filename)\n",
    "                f=inp.read()\n",
    "                words=word_tokenize(f)\n",
    "                words = [w.lower() for w in words]\n",
    "                noPunc = [w.translate(None,string.punctuation)\n",
    "                          for w in words]\n",
    "                noEmp = [w for w in noPunc if w.isalpha()]\n",
    "                noStop = [w for w in noEmp if not w\n",
    "                          in stop_words]\n",
    "                stemmed = [porter.stem(w) for w in noStop]\n",
    "                stemmed = [w for w in stemmed if not w\n",
    "                          in stop_words]\n",
    "            docLen+=len(stemmed)\n",
    "            docs.append(stemmed)\n",
    "            #docs.append(noStop)\n",
    "    D = len(docs)\n",
    "    print (\"Total Number of documents = %d\"%D)\n",
    "    print(\"Average words per document = %d\"%(docLen/D))\n",
    "    dcy = corpora.Dictionary(docs)\n",
    "    V = len(dcy)\n",
    "    print(\"Total vocabulary size = %d\"%V)\n",
    "    #dcy.save(os.path.join(TMP,'cong.dict'))\n",
    "    corpus = [dcy.doc2bow(text) for text in docs]\n",
    "    corpora.BleiCorpus.serialize(corpusfile,corpus)\n",
    "    M = matutils.corpus2dense(corpus, num_terms=V, num_docs=D,\n",
    "                              dtype=np.int32).T\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessWords(inputPath,corpusfile,stop_words):\n",
    "    '''\n",
    "    Parses all files in the inputPath folder and\n",
    "    returns the word matrix M:DxV of type ndarray(int32).\n",
    "    Also stores the corpus in blei's LDA-C format as \n",
    "    corpusfile (corpusfile is a full path with filename).\n",
    "    Input-specific stopwords also taken as array of strings\n",
    "    '''\n",
    "    porter = PorterStemmer()\n",
    "    docs,docLen=[],0\n",
    "    for path in inputPath:\n",
    "        print(\"Reading data from %s\"%path)\n",
    "        for filename in os.listdir(path):\n",
    "            with open(path+filename,'r') as inp:\n",
    "                f=inp.read()\n",
    "                words=word_tokenize(f)\n",
    "                words = [w.lower() for w in words]\n",
    "                noPunc = [w.translate(None,string.punctuation)\n",
    "                          for w in words]\n",
    "                noEmp = [w for w in noPunc if w.isalpha()]\n",
    "                noStop = [w for w in noEmp if not w\n",
    "                          in stop_words]\n",
    "                stemmed = [porter.stem(w) for w in noStop]\n",
    "                stemmed = [w for w in stemmed if not w\n",
    "                          in stop_words]\n",
    "            docLen+=len(stemmed)\n",
    "            docs.append(stemmed)\n",
    "            #docs.append(noStop)\n",
    "    D = len(docs)\n",
    "    print (\"Total Number of documents = %d\"%D)\n",
    "    print(\"Average words per document = %d\"%(docLen/D))\n",
    "    dcy = corpora.Dictionary(docs)\n",
    "    V = len(dcy)\n",
    "    print(\"Total vocabulary size = %d\"%V)\n",
    "    #dcy.save(os.path.join(TMP,'cong.dict'))\n",
    "    corpus = [dcy.doc2bow(text) for text in docs]\n",
    "    corpora.BleiCorpus.serialize(corpusfile,corpus)\n",
    "    M = matutils.corpus2dense(corpus, num_terms=V, num_docs=D,\n",
    "                              dtype=np.int32).T\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ./convote_v1.1/data_stage_one/development_set/\n",
      "Reading data from ./convote_v1.1/data_stage_one/training_set/\n",
      "Total Number of documents = 6362\n",
      "Average words per document = 118\n",
      "Total vocabulary size = 15547\n",
      "Reading phi\n",
      "Reading gamma\n",
      "Reading eta\n",
      "Time taken = 559.481253 sec\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['mr','would','say','lt', 'p', 'gt',\n",
    "               'amp', 'nbsp','bill','speaker','us',\n",
    "               'going','act','gentleman','gentlewoman',\n",
    "               'chairman','nay','yea','thank']\n",
    "pathnames = ['./convote_v1.1/data_stage_one/'+wor+'/'\n",
    "             for wor in ['development_set','training_set']]\n",
    "# Use development test(702 docs) only for debugging\n",
    "# i.e. Remove 'training set' from wor in pathnames\n",
    "pth = \"/Users/ishaan/MLPdatafiles\"\n",
    "# Create a path where you want to keep your output files\n",
    "os.system(\"rm -r \"+pth)\n",
    "# Ignore the os error that will come when no such file exists\n",
    "os.system(\"mkdir \"+pth)\n",
    "corpFile = pth+\"/congCorp.lda-c\"\n",
    "paramFolder = pth +\"/param\" \n",
    "alpha = 0.1\n",
    "K = 10\n",
    "M = preprocessWords(pathnames,corpFile,stop_words)\n",
    "eta,gamma,phi=findVariationalParams(M,corpFile,\n",
    "                                    paramFolder,alpha,K)\n",
    "t1=time.time()\n",
    "print (\"Time taken = %f sec\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envsparse",
   "language": "python",
   "name": "envsparse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
